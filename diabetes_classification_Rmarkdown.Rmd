---
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library('caret')
library(mice)
library(e1071)
library(rrcov)
library(corrplot)
library(MASS)
library("factoextra")
library(mclust)
library(dbscan)
library(class)
library(naivebayes)
library(nnet)
library(factoextra)
library('caret')
set.seed(10)
```

# Data set

This data set is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.

Columns	Description:

* Pregnancies-	To express the Number of pregnancies
* Glucose-	To express the Glucose level in blood
* BloodPressure (diastolic)-	To express the Blood pressure measurement (mmHg)
* SkinThickness-	To express the thickness of the skin (mm)
* Insulin-	To express the Insulin level in blood (U/mL)
* BMI-	To express the Body mass index (weight in kg/(height in m)^2)
* DiabetesPedigreeFunction-	To express the Diabetes percentage
(The pedigree provides a synthesis of the diabetes mellitus history in ancestors and the genetic relationship with the subject. It utilizes information from a personâ€™s family history to predict how likely a subject can get diabetes)
* Age-	To express the age (years)
* Outcome-	To express the final result 1 is Yes and 0 is No

This data contains information about year 1990.

Some important information is printed below:

```{r diabetes}
diabetes = read.csv("Diabetes.csv",sep = ",", dec = ".") #Read the csv data
attach(diabetes)
names(diabetes)  #Add column names
p = ncol(diabetes) - 1  #Number of features
p
n = nrow(diabetes)  #Number of rows
n
summary(diabetes) #Print a summary of the data and the features
```

## Graphical representation of the raw data


Below, the histograms of the quantitative features are presented .

```{r,fig.asp=0.5}
par(mfrow = c(1,2))
hist(Pregnancies, freq = F, nclass = 16)
hist(Glucose, freq = F, nclass = 16)
hist(BloodPressure, freq = F, nclass = 16)
hist(SkinThickness, freq = F, nclass = 16)
hist(Insulin, freq = F, nclass = 16)
hist(BMI, freq = F, nclass = 16)
hist(DiabetesPedigreeFunction, freq = F, nclass = 16)
hist(Age, freq = F, nclass = 16)

```

Also, a pie chart of the outcome binary variable is shown.

```{r,fig.asp=0.5}
pie(table(Outcome), labels = c('Non-diabetic','Diabetic'),edges=300,
    radius = 1,col=c('lightgrey','darkgrey'))
```


Some zero values seem to be out of the reasonable proportions or to be outliers. In the next step, will investigate which of these variables could or could not present zero values. Other values that will be investigated are pregnancies, as pregnancies above 10 which could imply some kind of outliers of the initial survey. In order to investigate this, the team will look up some Indian data, in particular of the Pima population.
Also, it can be seen that in the studied population, there is not a huge difference between diabetic and non-diabetic percentages, so no there is a good balance between the two classes in the study.

# Data imputation

Some of the variables are impossible to be zero, these are:

1. Glucose: A person with low glucose it is likely to pass out, he will surely die without glucose
2. Diastolic blood pressure: no blood pressure means no heartbeat
3. Skin thickness: 0 mm means no skin
4. Insulin: a person below 40 U/mL is likely to enter in coma
5. BMI: as it is defined, 0 means no weight
  
Implying that those zeros values that clearly show up in the histogram, must be imputed later.
   

```{r}
mean(Pregnancies)
sd(Pregnancies)

```

When evaluating the values of the feature Pregnancies, the mean of the population seems to be right according to the World bank data for 1990 <https://data.worldbank.org/indicator/SP.DYN.TFRT.IN?locations=IN&view=chart>, which states that the mean of the pregnancies per woman is about 4.

Other evaluation that can be carried out, is to compute if there is some abnormality with the age. In this case, the team propose to evaluate whether exist any instance in which Pregnancies*9/12 + 13 > Age.

```{r}
table(Pregnancies [Pregnancies*9/12 + 13 > Age])
```

# Imputing zero values with Predictive Mean Matching

First, the zero values are changed to "NA" when the zero is physically impossible. Later, the values are imputed using the library *"mice"*. The method used to impute is the Predictive Mean Matching:

$$x_{j} = \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{p} + \epsilon_{j} $$

The variables with the more values to impute are:

```{r}
NA_insulin <- sum(diabetes$Insulin==0)
NA_SkinThickness <- sum(diabetes$SkinThickness==0)
print(paste("The number of insulin missing values: ",NA_insulin))
print(paste("The number of insulin missing values: ",NA_SkinThickness))
```


```{r}
diabetes[diabetes$Glucose==0,"Glucose"] <- NA
diabetes[diabetes$BloodPressure==0,"BloodPressure"] <- NA
diabetes[diabetes$SkinThickness==0,"SkinThickness"] <- NA
diabetes[diabetes$Insulin==0,"Insulin"] <- NA
diabetes[diabetes$BMI==0,"BMI"] <- NA
diabetes[diabetes$Age==0,"Age"] <- NA

n_miss <- apply(is.na(diabetes),1,sum)  #Calculate the number of NA per column
table(n_miss)

diabetes_imp <- mice(diabetes,m=1,method="pmm")
diabetes_imp <- complete(diabetes_imp)
n <- nrow(diabetes_imp)

```
Now, almost every row have been imputed with the corresponding linear regression. Some few rows have been removed because still with the regression prediction, no reasonable physical values have been generated.

The resulting histograms for the imputed variables are shown below:

```{r,  fig.asp = 0.5}
par(mfrow = c(1,2))
hist(diabetes_imp$Pregnancies, freq = F,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)
hist(diabetes_imp$Glucose, freq = F, nclass = 16,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)
hist(diabetes_imp$BloodPressure, freq = F, nclass = 16,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)
hist(diabetes_imp$SkinThickness, freq = F, nclass = 16,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)
hist(diabetes_imp$Insulin, freq = F, nclass = 16,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)
hist(diabetes_imp$BMI, freq = F, nclass = 16,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)
hist(diabetes_imp$DiabetesPedigreeFunction, freq = F, nclass = 16,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)
hist(diabetes_imp$Age, freq = F, nclass = 16,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)
```

# Variable transformation

Looking at the histograms, it is clear that there are some variables with highly right-skewed. In order to ease the posterior analysis, we will take logarithms or square root transformation when it would be convenient.

$$ skew(X) \geq\ skew(Log(X)) + 0.20  $$ then X will be transformed.
$$ skew(X) < skew(Log(X)) + 0.20$$ then X will not be transformed.

```{r}

vect<- list(diabetes_imp$Pregnancies,
           diabetes_imp$Glucose,
           diabetes_imp$BloodPressure,
           diabetes_imp$SkinThickness,
           diabetes_imp$Insulin,
           diabetes_imp$BMI,
           diabetes_imp$DiabetesPedigreeFunction,
           diabetes_imp$Age)
sk <- lapply(vect,FUN=skewness)

```


```{r}

vect<- list(sqrt(diabetes_imp$Pregnancies),
           log(diabetes_imp$Glucose),
           log(diabetes_imp$BloodPressure),
           log(diabetes_imp$SkinThickness),
           log(diabetes_imp$Insulin),
           log(diabetes_imp$BMI),
           log(diabetes_imp$DiabetesPedigreeFunction),
           log(diabetes_imp$Age))
sk2 <- lapply(vect,FUN=skewness)

headers <- names(diabetes_imp)

#For every feature, we compare the skewness of the transformed variables vs the variable
for (i in 1:length(sk)){
  if (abs(sk[[i]])<=abs(sk2[[i]])+0.2){
    print(paste(headers[i],": Do not make transformation",sep=""))
  }
  else {
    print(paste(headers[i],": Make transformation",sep=""))
  }
  print(sk[[i]])
  print(sk2[[i]])
}

```
For the resulting variables, the histograms are shown below.

```{r,  fig.asp = 0.5}

par(mfrow = c(1,2))


diabetes_imp$SqrtPregnancies  <- sqrt(diabetes_imp$Pregnancies)
hist(diabetes_imp$SqrtPregnancies,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)

diabetes_imp$LogGlucose <- log(diabetes_imp$Glucose)
hist(diabetes_imp$LogGlucose,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)

diabetes_imp$LogInsulin  <- log(diabetes_imp$Insulin)
hist(diabetes_imp$LogInsulin,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)

diabetes_imp$LogBMI  <- log(diabetes_imp$BMI)
hist(diabetes_imp$LogBMI,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)

diabetes_imp$LogDiabetesPedigreeFunction  <- log(diabetes_imp$DiabetesPedigreeFunction)
hist(diabetes_imp$LogDiabetesPedigreeFunction,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)

diabetes_imp$LogAge  <- log(diabetes_imp$Age)
hist(diabetes_imp$LogAge,cex.main=0.7,cex.sub=0.7,cex.lab=0.7,cex.axis=0.7)
```

## Identifying outliers

Besides the values imputed before, which were non possible zeros for physical values, it is key to identify the observations that may introduce some errors to the model coming from some failures in the measure of the data. With the transformed data, it will be performed the Mahalanobis distance to every point, setting a threshold for outliers.

**1-** Mean vector, correlation matrix and covariance matrix calculations:

```{r}

diabetes_trans <- diabetes_imp[,c("SqrtPregnancies",
                  "LogGlucose",
                  "BloodPressure",
                  "SkinThickness",
                  "LogInsulin",
                  "LogBMI",
                  "LogDiabetesPedigreeFunction",
                  "LogAge",
                  "Outcome")]

colMeans(diabetes_trans)  #Means of every variable after the imputation
COV <- cov(diabetes_trans)
COV
COR <- cor(diabetes_trans)       #Correlation matrix
COR
```

**2-** Generate some graphs to see visually the correlations between the variables. It could also be used to look for outliers (typically points quite far from the masses).

```{r,  fig.asp = 1.5}

color_1 <- "deepskyblue2"
color_2 <- "seagreen2"
color_3 <- "orange2"

pairs(diabetes_trans[,-9],pch=16,cex=0.5,col=color_1)

```

For instance, when looking at the correlation plots, it can be seen that the variables SkinThinkness and LodBMI have a strong positive correlation, as well as LogGlucose and LogInsulin. Then, it can be quantified looking at the correlation matrix.

**3-** Select the 85% of points which are "more centered", and compute the correlation matrix (MCD) with those points. In this situation, we avoid noise imposed by the outliers, which might change the results of the matrix.


```{r,  fig.asp = 0.6}
MCD_est <- CovMcd(diabetes_trans[,-9],alpha=0.85,nsamp="deterministic")
m_MCD <- MCD_est$center
m_MCD
S_MCD <- MCD_est$cov
S_MCD
R_MCD <- cov2cor(S_MCD)
R_MCD

corrplot(R_MCD,mar=c(0,0,1,0),title = "Correlation plot for the 85% most centered data")

```

**4-** The main step is to compute the Mahalanobis distance. After this MCD matrix is computed, the Mahalanobis distance is calculated for every row. The outliers are the points with a distance greater than 95^(1/n) th quantile of the chi-square distribution: 

$$ X \sim N (\mu, \Sigma), \quad D_{M} \sim X_{p}^2 $$

```{r,  fig.asp = .6}

diabetes_sq_Mah_MCD <- MCD_est$mah
col_outliers_Mah_MCD <- rep(color_2,n)
outliers_Mah_MCD <- which(diabetes_sq_Mah_MCD>qchisq(.99^(1/n),p))
outliers_Mah_MCD
col_outliers_Mah_MCD[outliers_Mah_MCD] <- color_3
par(mfrow=c(1,2))
plot(1:n,diabetes_sq_Mah_MCD,pch=19,col=col_outliers_Mah_MCD,
     main="Squared Mahalanobis distances",xlab="Observation",ylab="Squared Mahalanobis distance")
abline(h=qchisq(.99^(1/n),p),lwd=3,col=color_1)
plot(1:n,log(diabetes_sq_Mah_MCD),pch=19,col=col_outliers_Mah_MCD,
     main="Log of squared Mahalanobis distances",
     xlab="Observation",ylab="Log of squared Mahalanobis distance")
abline(h=log(qchisq(.99^(1/n),p)),lwd=3,col=color_1)

pairs(diabetes_trans[,-9],pch=16,col=col_outliers_Mah_MCD,cex=0.35)

```

After identifying the outliers, those are removed from the data matrix. Finally the correlation and covariance matrix can be calculated correctly.

```{r}
diabetes_trans <- diabetes_trans[-outliers_Mah_MCD,]
n <- nrow(diabetes_trans)
n
```

A final computation of the covariance matrix is shown below differentiating each group by outcome of the diabetes. This is done in order to find out if there is any hidden correlation for each group when looking at the matrix for the whole data. 

```{r, fig.asp=.7}

par(mfrow = c(1,2))


df_quant <- diabetes_trans[,-9]
df0 <- diabetes_trans[diabetes_trans$Outcome == 0,]
df0_quant <- df0[,-9]
df1 <- diabetes_trans[diabetes_trans$Outcome == 1,]
df1_quant <- df1[,-9]

print("Global data")
print(colMeans(diabetes_trans))
print("Outcome = 0")
print(colMeans(df0))
print("Outcome = 1")
print(colMeans(df1))

S_0 <- cor(df0[,-9])
S_1 <- cor(df1[,-9])
```

```{r,  fig.asp = 0.6}
corrplot(S_0,title="Correlation plot for Non-Diabetic people",mar=c(0,0,1,0))
```

```{r,  fig.asp = 0.6}
corrplot(S_1,title="Correlation plot for Diabetic people",mar=c(0,0,1,0))
```

The correlation plots show that the correlation between variables is more or less the same for every pair of variables. For example, one thing that it can be pointed out, is that the correlations are slightly stronger for the non-diabetic people (outcome = 0) than for the diabetic people (outcome = 1)

## Data visualization

First, we will use some visual tools to identify trends for the outcome. The parallel coordinates are shown below.

```{r, warning=FALSE,  out.width="75%",fig.align = 'center'}

par(mfrow=c(1,1))

parcoord(df_quant,col=color_1,var.label=TRUE,main="PCP for diabetes data set", cex=0.5)


diabetes_colors <- c("red","blue")[1*(diabetes_trans$Outcome==0)+1]


parcoord(df_quant,col=diabetes_colors,var.label=TRUE,
         main="PCP for diabetes data set, separated by groups")

```

Now, we show the parallel coordinates plot separating by groups, in red non-diabetic and in blue diabetic.

```{r,fig.asp=0.5}
par(mfrow = c(1,2))

hist(df0$SqrtPregnancies, freq = F, col="blue")
hist(df1$SqrtPregnancies, freq = F, col="red")

hist(df0$LogGlucose, freq = F, col="blue")
hist(df1$LogGlucose, freq = F, col="red")

hist(df0$BloodPressure, freq = F, col="blue")
hist(df1$BloodPressure, freq = F, col="red")

hist(df0$SkinThickness, freq = F, col="blue")
hist(df1$SkinThickness, freq = F, col="red")

hist(df0$LogInsulin, freq = F, col="blue")
hist(df1$LogInsulin, freq = F, col="red")

hist(df0$LogBMI, freq = F, col="blue")
hist(df1$LogBMI, freq = F, col="red")

hist(df0$LogDiabetesPedigreeFunction, freq = F, col="blue")
hist(df1$LogDiabetesPedigreeFunction, freq = F, col="red")

hist(df0$LogAge, freq = F, col="blue")
hist(df1$LogAge, freq = F, col="red")

```


Taking a look at the graphs, in red, we have represented the histograms for the variables for the non-diabetic people, while at the left in blue, diabetic people are shown. The differences that are noticed are:
- Higher concentration of glucose for diabetics. This is something that can be previously supposed, as the diabetes is an illness that affects the biological generation of insulin which lowers the glucose in blood.
- Insulin is higher for diabetic people. This point is could be the consequence of two factors: almost half of the insulin variable is imputed and maybe some of those diabetic people are already diagnosed, implying that many of them could be taking synthetic insulin.
- Age distribution is different for diabetic and non-diabetic people. This could be affected for instance, by the age of which every people discover that he/she has diabetes.



## Principal component analysis

In the steps below, it will be calculated the principal component analysis for the whole data set, and then visualize the patterns in every principal component, for each group of diabetes.


```{r, out.width="75%",fig.align = 'center'}

df_quant_pcs = prcomp(df_quant,scale=TRUE,center=TRUE)

df_quant_pcs

par(mfrow=c(1,1))
plot(df_quant_pcs$x[,1:2],pch=19,col=diabetes_colors)

#See loadings for first PC
plot(1:8,df_quant_pcs$rotation[,1],pch=19,col="blue",main="Loadings for the first PC",
     xlab="Variables",ylab="Score",ylim=c(-1,1))
abline(h=0,col="red")
text(1:8,df_quant_pcs$rotation[,1],labels=colnames(df_quant),pos=1,col="black",cex=0.75)

#See loadings for second PC
plot(1:8,df_quant_pcs$rotation[,2],pch=19,col="blue",main="Loadings for the second PC",
     xlab="Variables",ylab="Score",ylim=c(-1,1))
abline(h=0,col="red")
text(1:8,df_quant_pcs$rotation[,2],labels=colnames(df_quant),pos=1,col="black",cex=0.75)

```
When watching the graphs above, it is clear that the two groups can be differentiated in the first principal component and are mixed in the second one. Also, looking at the loading, the first PC is not clear about the meaning, though it could be used as some metric to identify diabetes for the most negative values. On the other hand, the second PC is a metric of the maturity of a person, age and pregnancies are the most considered variables in this case.


```{r, out.width="75%",fig.align = 'center'}
plot(df_quant_pcs$rotation[,1:2],pch=19,col="red",main="Weights for the first two PCs",
     xlim=c(-1,1),ylim=c(-1,1))
abline(h=0,v=0)
text(df_quant_pcs$rotation[,1:2],labels=colnames(df_quant),pos=1,col="black",cex=0.75)
library(plotrix)
draw.circle(0,0,0.4,border="blue",lwd=3)
```

```{r}
biplot(df_quant_pcs,col=c("lightgrey","black"),cex=c(0.5,0.8))
```


Then, performing the analysis the percentage of the variance explained is seen in the graph. One thing that it could be pointed out, is that as the number of features here is not extremely large, and there are not highly correlated variables, the variance explained by the first 2 o 3 principal components, are not enough to obtain some conclusive results of the whole data set.

For instance, taking the first 3 PCs, it explains 65.6 % of the variance, using 37.5 % of the variables. This not as useful as in other cases with high correlation and high number of variables.


```{r, warning=FALSE,  out.width="75%",fig.align = 'center', message=FALSE}

df_quant_pcs$sdev^2

fviz_eig(df_quant_pcs,ncp=17,addlabels=T,barfill=color_1,barcolor=color_2)

get_eigenvalue(df_quant_pcs)

```

Using the elbow method, it can be selected the optimal number of PCs to be used in the analysis. In this case, the number of PCs could be 3 or 5. We will use 5 PCs in the following steps.

```{r, fig.asp=1.5}

pairs(df_quant_pcs$x[,1:5],col=diabetes_colors,pch=19,main="The first five PCs")


```

By generating the correlation plots, it is clear that the first principal component is a measure of the diabetes. It tends to assign lower values for patients with diabetes than non-diabetic patients. Other thing that can be pointed out in the first PC, is that the blue points (non-diabetic) seems to be equally distributed along the range with sort of a normal distribution. On the other hand, the red points (diabetic) are tending to right (between values of -4 and 0).

```{r}
corrplot(cor(df_quant,df_quant_pcs$x[,1:5]),is.corr=T)

```

In addition to the grapsh of loadings, a correlation plot can be used to have a notion of the loadings in every PC. It is very relevant that the first principal component is highly negative correlated with all the variables that are related to the diabetes.

# Independent Component Analysis

In the independent component analysis, we are obtaining variables which are independent from each other and follow a non-Gaussian distribution (skewness and kurtosis coefficient to be maximized).

```{r, message=FALSE}
library("ica")
diabetes_quant_ica <- icafast(df_quant,nc=8,alg="par")

Z <- diabetes_quant_ica$S
dim(Z)
head(Z)
colnames(Z) <- sprintf("IC-%d",seq(1,8))

cov(Z)
Z_norm <- Z * sqrt((n-1)/n)
cov(Z_norm)

```
```{r, fig.asp=1}
par(mfrow=c(4,2))
sapply(colnames(Z),function(cname){hist(as.data.frame(Z)[[cname]],
                                  main=cname,xlab="")})
```

When obtaining the ICs, the histograms are not shown to be highly skewed, but it can also be because that is generating lot of points far from the mean.

```{r, fig.asp=1}

par(mfrow=c(8,8))

neg_entropy <- function(z){1/12 * mean(z^3)^2 + 1/48 * mean(z^4)^2}
Z_neg_entropy <- apply(Z,2,neg_entropy)
ic_sort <- sort(Z_neg_entropy,decreasing=TRUE,index.return=TRUE)$ix
ic_sort
Z_ic_imp <- Z[,ic_sort]

pairs(Z_ic_imp[,1:8],col=diabetes_colors,pch=19,main="Correlation between ICs")

```

First groups of independent components  which have more entropy, shows observations which are far from the means of the variables. Of course, in this case outliers have been removed, still are some points far from the mean which are not classified as outliers by our algorithm presented above. The IC with less entropy, are separating the groups, something similat as the first PC of the 


```{r, fig.asp=0.5}
par(mfrow=c(1,2))
plot(Z_ic_imp[,1:2],pch=19,col=diabetes_colors,
     xlab="First IC",ylab="Second IC",main="First two IC scores")

plot(Z_ic_imp[,7:8],pch=19,col=diabetes_colors,
     xlab="Seventh IC",ylab="Eighth IC",main="Last two IC scores")

```


```{r}

corrplot(cor(df_quant_pcs$x,Z_ic_imp),is.corr=T)


```
The correlation between PCs and ICs is shown in the graph shows high negative correlation between PC-1 and independent components with lower entropy, which it has been pointed out that are separating the groups.

## Factor analysis

**1-** Estimation of the parameters $M_{0}$ and $\Sigma_{\nu}$ from the original data.

**2-** Estimation of the parameter $M_{1}$ .

**3-** Final estimation of the $M$ matrix with the varimax criterion. This criterion makes that the factors are highly correlated with some variables while being non-correlated with all the others. 

For this assignment, having eight variables, two factors have been considered for this method.

```{r, fig.width=4}

df_scale = scale(df_quant)
df_scale_pc <- prcomp(df_scale)
r <- 2  #Choose this number bc explains variance with PCs

#Estimation of M and Sig_nu
M_0 <- df_scale_pc$rotation[,1:r] %*% diag(df_scale_pc$sdev[1:r])  #Use sdev as it is standard deviation the sqrt(sigma)
M_0
S <- cov(df_scale)
Sigma_nu_0 <- diag(diag(S - M_0 %*% t(M_0)))
Sigma_nu_0

#Computing MM'
MM <- S - Sigma_nu_0
MM_eig <- eigen(MM)
MM_values <- MM_eig$values
MM_vectors <- MM_eig$vectors
M_1 <- MM_eig$vectors[,1:r] %*% diag(MM_eig$values[1:r])^(1/2)
M_1

#Last estimation
M <- varimax(M_1)
M <- loadings(M)[1:8,1:r]
M
Sigma_nu <- diag(diag(S - M %*% t(M)))
Sigma_nu

#Factor calculations
Fact <- df_scale %*% solve(Sigma_nu) %*% M %*% solve(t(M) %*% solve(Sigma_nu) %*% M)
pairs(Fact,pch=19,col=diabetes_colors)
```


```{r}
#Error calculations
Nu <- df_scale - Fact %*% t(M)
corrplot(cor(Nu),order="hclust")

```


When looking at the correlation plot for the error matrix, it is clear that the factor model is not explaining all the existing correlation between the variables. For instance, there is a high correlation between the errors of the features $\sqrt(Pregnancies)$ and $log(Age)$.

Another important conclusion, is that those error correlations do not depend on the number of factors for this data set. A bigger amount of factors have been tried, with similar or worse results in terms of correlation among errors.

Lastly, the uniqueness and communality are calculated for each variable. The communalities, are the ratio of the variance explained by the factors, while the uniqueness is the ratio of the variance which is not explained by the factors.

```{r, out.width="75%",fig.align = 'center'}
comM <- diag(M %*% t(M))
comM
names(comM) <- colnames(df_scale)
plot(1:p,sort(comM,decreasing=TRUE),pch=20,col="red",xlim=c(0,9),
     xlab="Variables",ylab="Communalities",main="Communalities")
text(1:p,sort(comM,decreasing=TRUE),labels=names(sort(comM,decreasing=TRUE)),
     pos=4,col="black")

uniqM <- 1 - comM
uniqM
plot(1:p,sort(uniqM,decreasing=TRUE),pch=20,col="red",xlim=c(0,9),
     xlab="Variables",ylab="Uniquenesses",main="Uniquenesses")
text(1:p,sort(uniqM,decreasing=TRUE),labels=names(sort(uniqM,decreasing=TRUE)),
     pos=4,col="black")
```

In this particular case, the two factors calculated, explain reasonably $Log(Age)$ and $Log(BMI)$ while do a poor job with $Log(DiabetesPedigree)$ and $BloodPreassure$.


# Unsupervised clasification

The aim of unsupervised classification is to see whether or not the groups obtained resemble those defined by the categorical variable outcome, diabetics and non-diabetics. 

We only use the quantitative variables to see what groups are formed and for checking if there are other characteristics in the sample that are not covered by outcome.

```{r}
X <- as.data.frame(df_scale)
Y <- diabetes_trans$Outcome 

n <- nrow(X)
p<-ncol(X)
```

## K-Means

The first unsupervised method is K-Means algorithm. First, we have to decide the optimal number of clusters for this dataset. The desired situation is that the optimal number of cluster is two, and the algorithms can distinguish between diabetics and non-diabetics without considering the predictor variable. To calculate the optimal number of clusters we propose:

  * Maximizing the average shilouette
  * Maximizing the gap statistic (with one standard error and prioritizing small number of clusters)

We train K-Mean algorithm for k clusters from 1 to 10 and then compute the average silhouette and the gap statistic:

```{r, out.width="75%",fig.align = 'center'}
par(mfrow=c(1,1))
fviz_nbclust(X,kmeans,method="silhouette",k.max=10)
```

It is clear that the maximum average shilouette is reached at k=2 which is aligned with the cardinality of the response variable, this could imply that K-Means could be separating those classes.


```{r, out.width="75%",fig.align = 'center'}
library(cluster)
gap_stat <- clusGap(X,FUN=kmeans,K.max=10,B=100)
fviz_gap_stat(gap_stat,linecolor="steelblue",maxSE=list(method="firstSEmax",SE.factor=1))
```

In the case of the gap statistic, we select the value of K as the smallest k such that the value of Gap (k) is not more than 1 standard deviation error away from the first local maximum, in this case, it is K=2 aswell.

After deciding the cluster number (k=2), the algorithm is trained. 20 random starting points are selected for the method to assure the optimal convergence for this algorithm. We store the best classification of them.

```{r}
cl_kmeans <- kmeans(X,2,nstart=20)
cl_predict <- abs(cl_kmeans$cluster - 2)
```

Finally, we can compare the outcome of the model with the classes of the response to see how similar they are. One key point of this assumption, is that the outcome of the K-Means algorithm is an unsupervised classification, so the prediction of clusters may be switched (0 and 1 may not correspond to 0 and 1 in the response). Classes might be interchanged as the model does not have a reference number (0 or 1) to assign to each cluster, it is just detecting two different groups of instances.

```{r}
ER_kmeans = 1/n*sum(abs(Y-cl_predict))
ER_kmeans
confusionMatrix(data=as.factor(cl_predict), reference = as.factor(Y), positive = "1")
```

The resultant accuracy is around 0.70 which is greater than that obtained with the naive classifier that labels all instances as the majority class non-diabetic (0.65). Notice that the sensitivity of diabetic instances is really high (0.80) but the specificity is not very good (0.65).

Now, we represent the classification of the instances in the two obtained groups. With this purpose we make use of the first two principal components. Notice that the two groups are divided by the first PC. This is due to the employed metric is the euclidean one.

```{r, out.width="75%",fig.align = 'center'}
cluster_colors <- c("red","blue")[1*(cl_predict==0)+1]
plot(df_quant_pcs$x[,1],df_quant_pcs$x[,2],col=cluster_colors, xlab= "PC1", ylab="PC2",
     main = "Clustering result for K-Means", cex=1, pch=19 )

```

The K-Means is clearly finding two clusters which can be deduced by the first principal component. Recall that the first PC was the component in which we could see diabetics instances for negative values.


## K-Medoids

As K-Means is sensitive to outliers because it uses sample mean vectors as cluster centers and the Euclidean distance, we also try K-Medoids. In this case, the metric used is Manhattan distance and the centers will be the medoids. 

As in the previous case, we maximize the silhouette value trying different K values. The selected K is 2.


```{r, out.width="75%",fig.align = 'center'}
X_K <- matrix(NA,nrow=1,ncol=19)
for (i in 1:19){
  pam_X_Gower_mat <- pam(X,k=i+1,diss=FALSE, metric = "euclidean")
  X_K[i] <- pam_X_Gower_mat$silinfo$avg.width
}
plot(2:20,X_K,pch=19,col="deepskyblue2",xlab="Number of clusters for k-medoids",ylab="Average silhouette")
medoids_k <- which.max(X_K)+1
```

We use Partitioning Around Medoids (PAM) that is the standard K-Medoids algorithm to classify the instances in two different clusters. Then we compare with outcome classes to see if they keep any relationship.

```{r}
cl_medoids <- pam(X,k=medoids_k,diss=FALSE, metric= "manhattan")
cl_predict_med <- abs(cl_medoids$clustering - 2)
confusionMatrix(data=as.factor(cl_predict_med), reference = as.factor(Y), positive = "1")
ER_kmeans = 1/n*sum(abs(Y-cl_predict_med))
ER_kmeans
```

After performing the K-Medoids algorithm, we can compare the clusters given to the diabetics response. This method gives almost the same results than K-means in terms of accuracy, but results in terms of sensitivity and specificity vary greatly. It could be said that one of those methods could be selected When prioritizing one of the two metrics (sensitivity/specificity tradeoff). Looking at the graph, we can see that the clusters are separated by the first principal component as the previous case, but has some intrinsic noise inside.


```{r, out.width="75%",fig.align = 'center'}
cluster_colors <- c("red","blue")[1*(cl_predict_med==0)+1]
plot(df_quant_pcs$x[,1],df_quant_pcs$x[,2],col=cluster_colors, xlab= "PC1", ylab="PC2",
     main = "Clustering result for K-Medoids", cex=1, pch=19 )
```

## Hierarchical clustering

### Agglomerative algorithm

We start with an agglomerative algorithm. It starts with as many clusters as observations and merges them based on the metric. We try with Manhattan distance. Due to the high number of observations, the correspondent dendogram will not be very visual so we decide not to include it.

```{r}
man_dist_X <- daisy(X,metric="manhattan",stand=FALSE)
single_X <- hclust(man_dist_X,method="single")
cl_single_X <- cutree(single_X,2) - 1
```

```{r}
# Confusion table
confusionMatrix(data=as.factor(cl_single_X), reference = as.factor(Y), positive = "1")
ER_kmeans = 1/n*sum(abs(Y-cl_single_X))
ER_kmeans
```

Single linkage algorithm for agglomerative clustering seems to give poor results, as only one instance is belonging to the minority group. Although single linkage was the algorithm selected to perform agglomerative hierarchical clustering (for its capabilities of creating long clusters), it is clearly not performing well for this set.

### Divisive algorithm

The divisive algorithm starts with only one cluster and start dividing it.

```{r}
diana_X <- diana(X,metric="manhattan",diss=FALSE)
cl_diana_X <- abs(cutree(diana_X,2) - 2)

confusionMatrix(data=as.factor(cl_diana_X), reference = as.factor(Y), positive = "1")
ER_diana = 1/n*sum(abs(Y-cl_diana_X))
ER_diana
```

```{r, out.width="75%",fig.align = 'center'}
cluster_colors <- c("red","blue")[1*(cl_diana_X==0)+1]
plot(df_quant_pcs$x[,1],df_quant_pcs$x[,2],col=cluster_colors, xlab= "PC1", ylab="PC2",
     main = "Clustering result", cex=1, pch=19 )
```

The divisive algorithm ends up with similar results as the one seen from K-Means, separating clases with the first principal component. This particular case presents good accuracy for the true positive class and bad accuracy for the negative class.

## Model-based cluster

The idea for this unsupervised algorithm, is to group by model clusters. Those models are multivariate Gaussians with particular characteristics. Those multivariate gaussians are computed maximizing the likelihood. After the models are generated considering the different hypothesis, the model with maximum Bayesian Information Criterion (BIC), is selected to cluster the instances.

The methodologies to implement are the following:

  * Ellipsoidal (full covariance matrix)
  * Diagonal (diagonal covariance matrix)

With different configurations of volumes and orientations.

```{r, out.width="75%",fig.align = 'center'}
BIC_X <- mclustBIC(X,G=1:7)
BIC_X

# Have a look at the results

plot(BIC_X)
```

The selected algorithm is the ellipsoidal, equal volume and orientation with two clusters.


```{r}
Mclust_X <- Mclust(X,x=BIC_X)$classification
cl_Mclust_X  <- abs(Mclust_X - 2)

confusionMatrix(data=as.factor(cl_Mclust_X ), reference = as.factor(Y), positive = "1")
ER_Mclust  = 1/n*sum(abs(Y-cl_Mclust_X ))
ER_Mclust 
```

Although the BIC is selected to perform the best model, the accuracy and the prediction of the true positive class is very poor. 

```{r, out.width="75%",fig.align = 'center'}
cluster_colors <- c("red","blue")[1*(cl_Mclust_X==0)+1]
plot(df_quant_pcs$x[,1],df_quant_pcs$x[,2],col=cluster_colors, xlab= "PC1", ylab="PC2",
     main = "Clustering result", cex=1, pch=19 )
```

We can observe that the two groups are not well differentiated by the PC1 and what is more they are better determined by PC2. This is the reason that the makes the groups not match with diabetics and non-diabetics.

## Density-based (DBSCAN)

The density based clustering considers different regions in which the density of points is very large, while separating the classes in the zones of less density. Before constructing the model, it could be assumed that the model will not perform well, as the separation between classes does not happens in a low density zone, quite the opposite (see Principal Component Analysis section).

For this model we will assume $MP$ (minimum number of close observations) to 5, usual value fo that parameter. The threshold then is selected at the change in slope for the graph below:


```{r, out.width="75%",fig.align = 'center'}
minPts <- 5
kNNdistplot(X,k=minPts-1)
abline(a=1.14444,b=.000972,col="red")
abline(h=1.68, col="blue")
```
$$\epsilon = 1.68$$
After selecting the two parameters of the model, it can be constructed and every instance is assigned into a cluster.

```{r, out.width="75%",fig.align = 'center'}
dbscan_X<- dbscan(X,eps=1.68,minPts=minPts)
dbscan_X
colors_dbscan_X <- c("red","blue")[dbscan_X$cluster+1]
plot(df_quant_pcs$x[,1],df_quant_pcs$x[,2],pch=19,col=colors_dbscan_X,xlab="First PC",ylab="Second PC")
```

As it happened before, the algorithm struggles to find the two clusters in which diabetics and non-diabetics are contained, instead, the model just finds one unique cluster and many points which are noisy instances (points far from the mean vector).


# Supervised clasification

There are many  methods available to perform supervised classification and no method dominates all over all possible data sets, so we need to try every different method to compare the different performances.

we are going to make use of the outcome variable to try to get better classifications of the instances between diabetics and non-diabetics. For this reason, we divide the data set in training sample for tuning the parameters and test sample for evaluating the model performance (holdout). The classic proportions are used in the split, 70% for training and 30% for testing.

```{r}
n_train <- floor(.7*n)
n_test <- n - n_train
shuf <- sample(1:n,n)
i_train<-shuf[1:n_train]
i_test<-shuf[(n_train+1):n]
X_train <- X[i_train,]
Y_train <- Y[i_train]
X_test <- X[i_test,]
Y_test <- Y[i_test]
```

## KNN

The first supervised algorithm we are studying will be K-Nearest Neighbors (KNN) which classifies the instances according to the most common class among its neighbors. We use LOOCV (Leave one out cross validation) for optimizing the parameter K referring to the number of neighbors considered by minimizing the error rate.

```{r, out.width="75%",fig.align = 'center'}
LER <- rep(NA,40)
for (i in 3 : 40){
  knn_output <- knn.cv(X_train,Y_train,k=i)
  LER[i] <- 1 - mean(knn_output==Y_train)
}
plot(1:40,LER,pch=20,col=color_1,type="b",
     xlab="k",ylab="LER",main="LER for logs of spam data set")

# Take k as the one that gives the minimum LOOCV error rate

k <- which.min(LER)
k

```
After defining the number of neighbours, the final model can be constructed with the training test. After constructing the model, it is evaluated using the test set. The best value for the parameter K is 31 neighbors to consider. We represent the confusion matrix and the more representative statistics for the classification.

```{r}
# Classify the responses in the test data set with the k selected
knn_Y_test <- knn(X_train,X_test,Y_train,k=k,prob=T)

confusionMatrix(data=as.factor(knn_Y_test), reference = as.factor(Y_test), positive = "1")
knn_num = as.numeric(knn_Y_test) - 1 
LER_KNN  = 1/n_test*sum(abs(Y_test-knn_num))
LER_KNN 
```

The results improve greatly against the unsupervised ones. The overall accuracy is 0.76 and the specificity is 0.92.
The problem is that sensitivity is only 0.46 which implies that less than half of diabetics group is found. This is due to the positive class is minority. 

We show the performance of the classification against the winning probabilities. As it was expected the vast proportion of the errors are produced when the probabilities are near to the threshold 0.5 which decides the class to label.

```{r, out.width="75%",fig.align = 'center'}
prob_knn_Y_test <- attributes(knn_Y_test)$prob

# In green, good classifications, in red, wrong classifications

colors_errors <- c("red","green")[1*(Y_test==knn_Y_test)+1]
plot(1:n_test,prob_knn_Y_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Winning probabilities",main="Winning probabilities")
```
Note that one way to improve this model, or at least improve the sensitivity (at specificity cost) could be to tune the threshold and generate a more balanced model.

## Methods based on Bayes Theorem

The conditional probabilities for belonging an instance to a given class can be computed thanks to the Bayes theorem. With this aim, prior probabilities and the density function of each class must be estimated. We are assuming that the densities are multivariate Gaussian, which gives rise to the linear and quadratic classifiers.
 

### Linear discriminant analysis - LDA

In this method, we assume that the mean vectors of the two classes are different but the covariance matrix is the same. We estimate the hyper-parameters prior probabilities $\pi_0,\pi_1$, mean vectors $\mu_0, \mu_1$ and covariance matrix $\Sigma$ in the training sample. Then we evaluate in the training set the performance of the model.

```{r}
# Estimate the unknown parameters with the training sample
lda_train <- lda(Y_train~.,data=as.data.frame(X_train))

# Classify the observations in the test sample
lda_test <- predict(lda_train,newdata=as.data.frame(X_test))

# The vector of classifications made can be found here
lda_Y_test <- lda_test$class

confusionMatrix(data=lda_Y_test, reference = as.factor(Y_test), positive = "1")
LDA_num = as.numeric(lda_Y_test) - 1
TER_LDA  = 1/n_test*sum(abs(Y_test-LDA_num))
TER_LDA 

```
The accuracy is almost the same as the previous obtained with KNN. Although the specificity is reduced a bit, the sensitivity grows to 0.53 which still is considered to be a bad result.

We plot the instances against the winning probabilities. As it was expected the red points representing the miss classifications are gathered around the central line.

```{r, out.width="75%",fig.align = 'center'}
# Conditional probabilities of the classifications made with the test sample
prob_lda_Y_test <- lda_test$posterior
colors_errors <- c("red","green")[1*(Y_test==LDA_num)+1]
plot(1:n_test,prob_lda_Y_test[,2],col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of diabetes",
     main="Probabilities of diabetes")
abline(h=0.5)
```

### Quadratic discriminant analysis - QDA

In this method, we assume that the mean vectors and the covariance matrices of the two classes are different. We estimate the hyper-parameters prior probabilities $\pi_0,\pi_1$, mean vectors $\mu_0, \mu_1$ and covariance matrices $\Sigma_0, \Sigma_1$ in the training sample. Then we evaluate in the training set the performance of the model.

```{r}
# Estimate the unknown parameters with the training sample

qda_train <- qda(Y_train~.,data=as.data.frame(X_train))

# Classify the observations in the test sample

qda_test <- predict(qda_train,newdata=as.data.frame(X_test))

# The vector of classifications made can be found here

qda_Y_test <- qda_test$class

confusionMatrix(data=qda_Y_test, reference = as.factor(Y_test), positive = "1")
QDA_num = as.numeric(qda_Y_test) - 1
TER_QDA  = 1/n_test*sum(abs(Y_test-QDA_num))
TER_QDA 

```

We observe very similar results than the ones seen in the previous model (LDA). This better classification of the positive instances is derived by considering a specific covariance matrix for the diabetic population.

We plot the conditional probabilities of winning. In green the successes and in red the failures.

```{r, out.width="75%",fig.align = 'center'}
# Conditional probabilities of the classifications made with the test sample
prob_qda_Y_test <- qda_test$posterior
colors_errors <- c("red","green")[1*(Y_test==QDA_num)+1]
plot(1:n_test,prob_qda_Y_test[,2],col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of diabetes",
     main="Probabilities of diabetes")
abline(h=0.5)
```

### Naive Bayes

This method assumes that the predictors are independent variables. Therefore, the variances are estimated but the covariances are assumed to be just 0, so there is no need to estimate them. This can be justified in terms of computational flexibility but, Naive Bayes should perform worse than both LDA and QDA.

We estimate the parameters with the training sample. In this case the covariance matrix will be diagonal saving computational time.

```{r}
nb_train <- gaussian_naive_bayes(as.matrix(X_train),as.factor(Y_train))

# Classify the observations in the test sample

nb_test <- predict(nb_train,newdata= as.matrix(X_test),type="prob")
nb_Y_test <- c(0,1)[apply(nb_test,1,which.max)]

```

```{r}
confusionMatrix(data=as.factor(nb_Y_test), reference = as.factor(Y_test),positive = "1")
nb_num = as.numeric(nb_Y_test) - 1
TER_NB  = 1/n_test*sum(abs(Y_test-nb_num))
TER_NB 
```

As expected, Naive Bayes obtains worse results than the previous methods, will a very low sensitivity, which seems to be the worse defect of all these methods.


### Logistic regression

We employ logistic regression to classify the instances by its probabilities. First we fit the model in the training set to estimate the regression coefficients $\beta_0,\dots\beta_8$ and then we evaluate in the test set.

```{r}
# Estimate the parameters of the logistic regression model with the training sample
lr_train <- multinom(Y_train~.,data=as.data.frame(X_train))

# Classify the responses in the test data set and estimate the test error rate
lr_test <- predict(lr_train,newdata=as.data.frame(X_test))

confusionMatrix(data=as.factor(lr_test), reference = as.factor(Y_test),positive = "1")
lr_num = as.numeric(lr_test) - 1
TER_lr  = 1/n_test*sum(abs(Y_test-lr_num))
TER_lr
```

This model produces one of the best accuracy 0.76 but the sensitivity decreases to 0.54 so the diabetics group is still not well captured.

```{r, out.width="75%",fig.align = 'center'}
# Conditional probabilities of the classifications made with the test sample
prob_lr_test <- predict(lr_train,newdata=X_test,type ="probs")
colors_errors <- c("red","green")[1*(Y_test==lr_num)+1]
plot(1:n_test,prob_lr_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of diabetes",
     main="Probabilities of diabetes")
abline(h=0.5)
```

By means of a t-test, we rank the different coefficients score to order its relevance in the former model. As we can see LogInsulin is in the last position and it can be because it was a variable with many missing values and the imputation performed could not be perfect for this reason.

```{r}
# To see which are the most significant coefficients, we use the t-test that is computed next
t_test_lr_train <- summary(lr_train)$coefficients/summary(lr_train)$standard.errors

# Sort the absolute value of the t-test in decreasing order
sort(abs(t_test_lr_train),decreasing=TRUE)
```

Although we are not considering many different variables, we will perform a BIC selection to check if removing some of them we obtain a model with a better performance.


```{r, results='hide'}
# Try to improve the test error rate by deleting predictors without discriminatory power
step_lr_train <- step(lr_train,direction="backward",trace=1,  K= "log(n)")
```

After performing the BIC selection, those four variables are the selected predictors:

```{r}
#Show the selected predictors
step_lr_train$coefnames
```


```{r}
# Classify the responses in the test data set with this new model
step_lr_test <- predict(step_lr_train,newdata=X_test)

confusionMatrix(data=as.factor(step_lr_test), reference = as.factor(Y_test),positive = "1")
step_lr_num = as.numeric(step_lr_test) - 1
TER_lr  = 1/n_test*sum(abs(Y_test-step_lr_num))
TER_lr
```
We obtain similar results, so the final step is to tune the hyper-parameters of the model to get a more balanced model with similar specificity or sensitivity.

We obtain an improvement applying predictor selection method in the accuracy but the true positive rate of diabetes remains very low.

Finally, we will try to tune some hyper-parameters of the logistic model, in order to obtain a more balanced model. The hyper-parameter to tune will be the $F1-score$ which is a metric heavily used in unbalanced cases.

$$F1 = \frac{2*precision*recall}{precision+recall}$$
Where:

  * Recall: $\frac{tp}{tp+fp}$
  
  * Precision: $\frac{tp}{tp+fn}$
  
  and
  
$tp$:true positives, $fp$:false positives, $tn$:true negatives, $fn$:false negatives

The hyper-parameters to tune will be the threshold which classifies instances from the probabilities given by the model, and the weights in which instances are being considered. We will use the grid search (two nested 'for') method in order to obtain the best pair of parameter for maximizing the F1-score.

```{r, results='hide'}

F1_score = 0
threshold = 0
acc_tuned = 0
sens_tuned = 0
spec_tuned = 0

alpha_vec = seq(0.25,0.75,0.025)
w_vec <- c(4,4.5,5,5.5,6,6.5,7)

ind<-which(Y_train==1)
weights <- rep(1,length(Y_train))


for (alpha in alpha_vec){
  for (w in w_vec){
    weights[ind] <- w
    
    lr_hpo <- multinom(Y_train ~ LogDiabetesPedigreeFunction + LogAge + LogBMI + LogGlucose,
                       data=as.data.frame(X_train),weights = weights)
    Y_pred <- predict(lr_hpo, newdata=X_train, type='probs')
    H <- table(Y_pred > alpha, Y_train == 1) 
    tp = H[2,2]
    tn = H[1,1]
    fp = H[2,1]
    fn = H[1,2]
    total = tp+tn+fp+fn
    # Calculating F1-score
    acc = (tp+tn)/total
    pre = (tp)/(tp+fp)
    rec = (tp)/(tp+fn)
    F1 = 2*rec*pre/(pre+rec)
    
    if (F1>F1_score){
      F1_score <- F1
      threshold <- alpha
      w_tuned <- w
      acc_tuned <- acc
      sens_tuned <- rec
      spec_tuned <- tn/(tn+fp)
      Y_pred_tuned <- Y_pred
} } }
```

```{r}
#Print the results
print(paste('Threshold: ',threshold,' Weight: ',w_tuned, ' F1-score: ',F1_score, 'Accuracy: ', acc_tuned))
```

Now, lets train the optimized model and then evaluate it in the test set:

```{r}
weights[ind] <- w_tuned
# Estimate the parameters of the logistic regression model with the training sample
lr_hpo <- multinom(Y_train ~ LogDiabetesPedigreeFunction + LogAge + LogBMI + LogGlucose,
                   data=as.data.frame(X_train),weights = weights)

Y_prob_test <- predict(lr_hpo, newdata=X_test, type='probs')
Y_pred_test <- rep(0,length(Y_test))
indexs <- which(Y_prob_test > threshold)
Y_pred_test[indexs] <- 1    

confusionMatrix(data=as.factor(Y_pred_test), reference = as.factor(Y_test),positive = "1")
lr_num = as.numeric(lr_test) - 1
TER_lr  = 1/n_test*sum(abs(Y_test-lr_num))
TER_lr
```
After estimating the performance of the model with the test set, we can conclude that the sensitivity has grown to 0.73 while specificity has decreased to 0.72. The tuning of the F1-score implies to balance the model and both classes prediction, generally at the cost of the accuracy. In this particular case, the accuracy has decreased to 0.72, not consider a great loss in this metric.

One key point when dealing with unbalanced classes (we could say that this problem is unbalanced as one class has two times the instances as the other), is to define which of the metrics is to be maximized. Usually, missing some diabetic person is more costly than classify wrongly a non-diabetic, so the F-score can be tuned accordingly taking that difference into consideration. 

Finally, we can obtain the plot in which we can see the instances well classified in green and the instances wrong classified in red. It is clear that the model is miss classifying instances in the positive category, but with the benefit of capturing almost all the instances belonging to that class.

```{r, out.width="75%",fig.align = 'center'}
# Conditional probabilities of the classifications made with the test sample
colors_errors <- c("red","green")[1*(Y_test==Y_pred_test)+1]
plot(1:n_test,prob_lr_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of diabetes",
     main="Probabilities of diabetes")
abline(h=threshold)
```

\newpage

# Conclusions

After studying the data, transforming and visualizing and performing dimension reduction techniques, several supervised and unsupervised methods, we have reach to the following conclusions:

* The selected data set is slightly imbalanced so when it comes to evaluate the performance of the classification algorithms, we do not only take into account the accuracy but also a trade-off between sensitivity and specificity. 

* The existence of a large amount of NA values in insulin and skin thickness make the imputation not to be so accurate and therefore they are discarded in the variable selection.

* PCA and ICA allow a good visualization while factor analysis works poorly due to the small number of factors, correlation matrix of errors shows existing correlations among them that have not been explained by the model. In addition, we would like to highlight the capacity of PC1 to split the instances in diabetics and non-diabetics group.

* In unsupervised classification, both partitional clustering methods K-means and K-medoids yields to two clusters that considerably match with outcome classes. Within hierarchical clustering, the agglomerative method with single linkage produces one cluster with only one observation resembling to the naive classifier, majority class. Divisive algorithm results in a very balanced classification with satisfactory both sensitivity and specificity. In model-based clustering the resulting configuration was ellipsoidal with equal volume and orientation but leads to poor results. Similarly, selecting a threshold of 1.68 and minimum number of points 5, DBSCAN does not perform properly. The common point of all techniques is that all tend to clusterize in two groups. The better results obtained are around 0.7 in all accuracy, sensitivity and specificity.

* In supervised classification, we can see an improvement in comparison with the unsupervised methods. Still, the imbalanced class problem present difficulties when selecting the model. In this case, all the supervised algorithms were trained with very low sensitivity, meaning that the positive class was not well classified. Therefore, it is key to tune the hyper-parameters of the model in order to get the sensitivity-specificity trade off according to the problem needs. In this case, we maximized the F1-score for the logistic regression, obtaining balanced algorithms with similar values for sensitivity and specificity. 

* Finally, we could deduce that the non-diabetic people which are classified with high probabilities of having diabetes, should consider the possibility of keeping with medical consulting, as they might 'awake' the deceased in the future. This could be the reason for not getting a higher accuracy.






